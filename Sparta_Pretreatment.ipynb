{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNc8tJbjXWR8ko7AljqYyEA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/makeflower99/test/blob/master/Sparta_Pretreatment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZ0U4Itst0ma"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium\n",
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from konlpy.tag import Okt\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "import time\n",
        "from nltk import pos_tag\n",
        "#크롤링한 결과 불러오기\n",
        "\n",
        "#스파르타 크롤링 결과\n",
        "df1= pd.read_csv('/Sparta.csv')\n",
        "df1.columns=[0,'title']\n",
        "list1 = df1['title'].tolist()\n",
        "# 불용어 리스트 정의\n",
        "df2=pd.read_csv('/불용어리스트.csv')\n",
        "df2.columns=['name']\n",
        "list2 = df2['name'].tolist()\n",
        "\n",
        "\n",
        "# 추가적인 불용어 정의\n",
        "additional_stopwords = ['[', ']', '(', ')', ',', '.', '!', '?', '위한', '위']\n",
        "\n",
        "# 불용어 리스트 결합\n",
        "stopwords = list2 + additional_stopwords\n",
        "\n",
        "# 띄어쓰기를 기준으로 단어 분리 및 불용어 제거\n",
        "tokens = []\n",
        "for title in list1:\n",
        "    title = re.sub(r\"[^가-힣\\s]\", \"\", title)  # 한글과 공백을 제외한 모든 문자 제거\n",
        "    words = title.split()  # 띄어쓰기를 기준으로 단어 분리\n",
        "    for word in words:\n",
        "        if word not in stopwords:\n",
        "            tokens.append(word)\n",
        "\n",
        "# 가장 많이 나타나는 단어 추출\n",
        "word_count = Counter(tokens)\n",
        "most_common_words = word_count.most_common()\n",
        "\n",
        "# 형태소 분석기 초기화\n",
        "okt = Okt()\n",
        "\n",
        "# 제외하고 싶은 단어 리스트 정의\n",
        "exclude_words = ['다시보기','월간','만들기']\n",
        "\n",
        "# 명사에 포함될 단어 리스트 정의\n",
        "include_words = ['챌린지','시각화']\n",
        "\n",
        "# 추출된 단어들 중 명사 처리\n",
        "nouns_only_korean = {}\n",
        "for word, count in most_common_words:\n",
        "\n",
        "    # 제외 대상 단어인 경우 건너뛰기\n",
        "    if word in exclude_words:\n",
        "        continue\n",
        "    # 명사에 포함할 단어 예외 처리\n",
        "    if word in include_words:\n",
        "        nouns_only_korean[word] = count\n",
        "        continue\n",
        "\n",
        "    pos_list = okt.pos(word)\n",
        "\n",
        "    # 모든 형태소가 명사인 경우\n",
        "    if all(pos == 'Noun' for _, pos in pos_list):\n",
        "        nouns_only_korean[word] = count\n",
        "\n",
        "    # 하나라도 명사가 아닌 형태소가 있는 경우\n",
        "    else:\n",
        "        for morph, pos in pos_list:\n",
        "            if pos == 'Noun':\n",
        "                if morph in nouns_only_korean:\n",
        "                    # 이미 있는 명사는 빈도수를 더함\n",
        "                    nouns_only_korean[morph] += count\n",
        "                else:\n",
        "                    # 새 명사는 추가\n",
        "                    nouns_only_korean[morph] = count\n",
        "\n",
        "# nltk에서 필요한 자원 다운로드\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# 데이터 불러오기\n",
        "df1 = pd.read_csv('/Sparta.csv')\n",
        "df1.columns = [0, 'title']\n",
        "list1 = df1['title'].tolist()\n",
        "\n",
        "# 추가적인 불용어 정의 (영어 버전)\n",
        "stopwords_english = ['[', ']', '(', ')', ',', '.', '!', '?', 'and', 'the']\n",
        "\n",
        "# 텍스트를 소문자로 변환하지 않고 불용어 제거\n",
        "filtered_text = ''\n",
        "for title in list1:\n",
        "    title = re.sub(r\"[^a-zA-Z\\s]\", \"\", title)  # 영문자와 공백을 제외한 모든 문자 제거\n",
        "    words_english = title.split()  # 단어 분리 (공백을 기준으로)\n",
        "    filtered_words_english = []\n",
        "    for word in words_english:\n",
        "        if word not in stopwords_english:\n",
        "            filtered_words_english.append(word)\n",
        "    filtered_text += ' '.join(filtered_words_english) + ' '\n",
        "\n",
        "# 가장 많이 나타나는 단어 추출\n",
        "word_count_english = Counter(filtered_text.split())\n",
        "most_common_words_english = word_count_english.most_common()\n",
        "\n",
        "# 영어 형태소 분석 및 명사 추출\n",
        "nouns_only_english = {}\n",
        "for word, count in most_common_words_english:\n",
        "    tagged_word = pos_tag([word])\n",
        "    if tagged_word[0][1].startswith('NN'):  # 명사(NN, NNP, NNS 등)인 경우\n",
        "        nouns_only_english[word] = count\n",
        "\n",
        "\n",
        "nouns_only_combined = nouns_only_english.copy()  # nouns_only를 복사하여 새로운 딕셔너리 생성\n",
        "nouns_only_combined.update(nouns_only_korean)\n",
        "\n",
        "sorted_nouns_only_combined = sorted(nouns_only_combined.items(),key=lambda x: x[1], reverse=True)\n",
        "\n",
        "result_df = pd.DataFrame(sorted_nouns_only_combined, columns=['단어', '빈도수']).head(10)\n",
        "result_df.to_csv('Sparta_result.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0XAexX0uAoK",
        "outputId": "ab77cede-d403-4455-a377-9505549ca03e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SOTtVnEauAkr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}