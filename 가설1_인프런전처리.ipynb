{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKi3gvFw7Jc50wxZc0eqQe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/makeflower99/test/blob/master/%EA%B0%80%EC%84%A41_%EC%9D%B8%ED%94%84%EB%9F%B0%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium\n",
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "hwObpmW30Rh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqS_uwNU0Ao9"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "from konlpy.tag import Hannanum\n",
        "import re\n",
        "import pandas as pd\n",
        "import time\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#크롤링한 결과 불러오기\n",
        "# 'Inflearn.csv'파일과 '불용어리스트.csv'는 파일위치에서 최상위부분에 위치하여야한다.\n",
        "#파일 넣는 방법 좌측에 폴더를 클릭한 후\n",
        "# ..(상위폴더로 이동)을 선택한 후 파일 업로드 버튼을 눌러 업로드 한다.\n",
        "# 저장된 파일은 content폴더에 위치해있습니다.\n",
        "\n",
        "data_o = pd.read_csv(\"/Inflearn.csv\")\n",
        "data = data_o.copy()\n",
        "#  KoNLPy 사용\n",
        "# 사용법 확인하기\n",
        "def remove_special_characters(df):\n",
        "    for title in df.title:\n",
        "        df['title'] = df['title'].str.replace(r\"[^\\w\\s]\", \" \", regex=True)\n",
        "    return df\n",
        "\n",
        "data = remove_special_characters(data)\n",
        "\n",
        "# 문장에서 명사 추출\n",
        "hannanum = Hannanum()\n",
        "nouns = []\n",
        "for text in data['title']:\n",
        "    nouns.extend(hannanum.nouns(text))\n",
        "count = Counter(nouns)\n",
        "#명사 빈도 추출 - 상위 단어 20개 추출\n",
        "noun_list = count.most_common(20)\n",
        "#불용어 처리 - 불용어 제거 리스트?\n",
        "#숫자까지 제거 데이터 프레임 : df_done1\n",
        "#명사만 : nouns\n",
        "korean_stopwords_path = \"/불용어리스트.csv\"\n",
        "\n",
        "# 텍스트 파일을 오픈합니다.\n",
        "with open(korean_stopwords_path, encoding='utf-8') as f:\n",
        "\tstopwords = f.readlines()\n",
        "stopwords = [x.strip() for x in stopwords]\n",
        "meaningful_words = [w for w in nouns if not w in stopwords]\n",
        "count1 = Counter(meaningful_words)\n",
        "#명사 빈도 추출 - 상위 단어 100개 추출\n",
        "noun_list = count1.most_common(20)\n",
        "# 빈 데이터 프레임 생성\n",
        "df_fin = pd.DataFrame(columns=['word', 'count'])\n",
        "\n",
        "# frequent2 리스트에서 데이터 프레임에 데이터 추가\n",
        "for word, count in noun_list:\n",
        "    df_fin.loc[len(df_fin)] = [word, count]\n",
        "\n",
        "df_fin.to_csv(\"Inflearn_ko.csv\",index=False)\n",
        "\n",
        "data_en = data_o.copy()\n",
        "\n",
        "def remove_non_english(df):\n",
        "    df['title'] = df['title'].str.replace(r\"[^a-zA-Z\\s]\", \"\", regex=True)\n",
        "    return df\n",
        "data_en = remove_non_english(data_en)\n",
        "\n",
        "def remove_non_english(df):\n",
        "    df['title'] = df['title'].str.replace(r\"[^a-zA-Z\\s]\", \"\", regex=True)\n",
        "    return df\n",
        "data_en = remove_non_english(data_en)\n",
        "compile = re.compile(\"\\W+\")\n",
        "for i in range(len(data_en)):\n",
        "  a = compile.sub(\" \", data_en[\"title\"][i])  # \"text\" 열을 수정한다고 가정\n",
        "  data_en.loc[i, \"title\"] = a.lower()  # \"text\" 열의 값을 수정\n",
        "#불용어 제거\n",
        "#불용어 데이터 다운로드\n",
        "nltk.download('all')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_word_eng = set(stopwords.words('english'))\n",
        "line2 = [i for i in data_en['title'] if i not in stop_word_eng]\n",
        "#어간 추출 대신 표제어 추출(lemmatization)을 사용\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "token = RegexpTokenizer('[\\w]+')\n",
        "result_pre_lem = [token.tokenize(i) for i in line2]\n",
        "middle_pre_lem= [r for i in result_pre_lem for r in i]\n",
        "final_lem = [lemmatizer.lemmatize(i) for i in middle_pre_lem if not i in stop_word_eng] # 불용어 제거\n",
        "\n",
        "#표제어 빈도수\n",
        "english2 = pd.Series(final_lem).value_counts().head(20)\n",
        "\n",
        "# 표제어\n",
        "english2 = pd.Series(final_lem).value_counts().head(22)\n",
        "\n",
        "# 데이터 프레임으로 변환\n",
        "df = english2.reset_index(name=\"count\")\n",
        "\n",
        "# 열 이름 변경\n",
        "df.columns = [\"word\", \"count\"]\n",
        "\n",
        "# 결과 출력\n",
        "df = df[~df[\"word\"].isin([\"part\", \"j\"])]\n",
        "\n",
        "df.to_csv(\"Inflearn_en.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1bmKLRVV0Tf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ekey7pBK0TYN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}